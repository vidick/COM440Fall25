% !TeX root = ../main_only_solutions.tex

\chapter{From Imperfect Information to (Near) Perfect Security}

\begin{exercises}


\item {\bf Using the Pretty-Good Measurement}

\begin{enumerate}
\item The overall success probability is $\frac13 \bra{+} \rho _0 \ket{+}  + \frac13 \bra{-} \rho _2 \ket{-}  = \frac13$.

\item Bob's overall success probability is $\frac13\bra{0} \rho _0 \ket{0}  + \frac13\bra{1} \rho_2 \ket{1}  = \frac23$.

\item Let $\rho=\frac{1}{3}(\rho_0+\rho_1+\rho_2) = \frac{1}{2}\Id$. The elements of the pretty-good measurement are $M_ i = \frac13\rho ^{-1/2}\rho_i\rho ^{-1/2}$. Since $\rho = \frac{1}{2}\id$, we have that $\rho ^{-1/2} = \sqrt{2}\id$. The overall success probability is 
\[ \frac{1}{3}\sum_i \Tr(M_i\rho_i) = \frac{2}{9} \sum_i \Tr(\rho _ i^2) = \frac{2}{9}\left(1 + \frac{1}{2} + 1\right) = \frac{5}{9}\;.\]

\item We check that for each $i$, $\frac13 \rho _ i \leq \sigma = \frac13\id$. Indeed,
\[\frac13\rho _0 = \frac13\left( \begin{array}{cc}1&  0 \\ 0 & 0 \end{array} \right) \leq \frac13\id\;,\qquad \frac13\rho _1 =\frac16\id\leq \frac13\id\;,\qquad
\frac13\rho _2 = \frac13\left( \begin{array}{cc}0&  0 \\ 0 & 1 \end{array} \right) \leq \frac13\id\;.\]
Thus the upper bound on the guessing probability is thus $\operatorname {Tr}\sigma = \frac23$.

\item If the optimal measurement has POVM elements $\{M_i\}$, then we observe that 
\begin{align*}
\sum_i p_i  \Tr(M_i \sigma_i) &= \sum_i  \Tr( M_i \cdot (p_i\sigma_i))\\
&\leq \sum_i \Tr(M_i\sigma)\\
&= \Tr\Big(\Big(\sum_i M_i\Big)\sigma\Big)\\
&= \Tr(\sigma)\;.
\end{align*}
Here, for the second line we used that if $\rho\leq \sigma$ then for any positive semidefinite $M$, $\Tr(M(\sigma-\rho))\geq 0$, i.e.\ $\Tr(M\rho)\leq \Tr(M\sigma)$. 
\end{enumerate}


\item {\bf Deterministic Extractors on Bit-Fixing Sources}

\begin{enumerate}
\item We can think of generating $X_0$ by $n-t$ independent fair coin flips, so each of its strings occurs with equal probability $2^{t-n}$ and $\Hmin (X_0) = -\log{2^{t-n}}=n-t$.

\item The number of strings with an even number of $0$s is equal to the number of strings with an odd number of $0$s, so each of these is equal to $2^{n-1}$. Thus the min-entropy of $X_1$ is $-\log \frac{1}{2^{n-1}}=(n-1)$.

\item As before, think of generating $X_2$ through a series of independent fair coin flips: it is fully determined by $\frac {n}{2}$ of them and so $\Hmin(X_2)=\frac{n}{2}$.

\item Let us look at all the proposed answers consecutively. We're interested in finding which ones are not constant.
\begin{statements}
\item $f_1(X_1)$ --- true. The string $X_1$ can be seen as $n-1$ random bits followed by a bit that is fully determined by the previous $n-1$ bits. Since there are $n-1$ random bits, performing \(x_ L\cdot x_ R \) will generate a random bit. Notice that this is not uniformly random; for example, if $n=4$, then an output of $0$ is $3$ times more likely than an output of $1$.
\item $f_1(X_2)$ --- true. Since the first \(\frac{n}{2}\) bits of $X_2$ are the same as the second half, we have \(x_ L\cdot x_ R  = XOR(x_L) = XOR(x_R)\) which is a uniformly random bit since the strings \(x_L=x_R\) are random.
\item $f_2(X_0)$ --- true. Since the last \(n-k\) bits of $X_0$ are fully random, the XOR of the entire string will result in a uniformly random bit.
\item $f_2(X_1)$ --- false. Since the number of $0$'s in the string is known the parity of the string (computed by the XOR) is $0$ for $n$ even.
\item $f_2(X_2)$ --- false. Since the first \(\frac{n}{2}\) bits of $X_2$ are the same as the second half, the parity of the bit string is zero.
\end{statements}

\item The XOR of all of the bits is equal to $b\oplus r$, for $r$ equal to the XOR of the bits learned by Eve and $b$ equal to the XOR of all of the other bits. Regardless of the distribution of $r$, $b\oplus r$ is uniform independent of Eve's knowledge.

\item $t=n-1$. In this case there is at least one bit that Eve did not get access to. Call this bit $b$. From Eve's point of view, $b$ is uniformly distributed and independent of everything else. We only require the existence of one bit that Eve does not get access to.

\item Following the last question, each subsource must have at least $t+1$ bits. They can make at most $\lfloor \frac{n}{t+1}\rfloor$ such subsources.
\end{enumerate}


\item {\bf Conditional Min-Entropy}

\begin{enumerate}
\item XY is uniform on $2$ bits, so its min-entropy is $2$.

\item Since $X$ and $Y$ are independent, the min-entropy of $X$ conditioned on $Y$ is equal to the min-entropy of $X$. So $\Hmin(Y) = \Hmin(X| Y) = \Hmin (X) = 1$.

\item The highest probability outcome has probability $\frac12$, so the min-entropy is $-\log\frac12 = 1$.

\item The probability that $X =0$ is $\frac58$, so 
\[\Hmin(X) = -\log\frac58 = 3-\log 5 \approx 0.678\;.\]
Conditioned on $X = 0$, the most likely outcome for $Y$ is $Y = 0$ with probability $\frac45$. Conditioned on $X = 1$, the most likely outcome for $Y$ is $Y = 0$ with probability $\frac23$.
Then $P_\text{guess}(Y| X) = \frac58\cdot \frac45 + \frac38 \cdot \frac23 = \frac34$. So $\Hmin(Y| X) = -\log \frac34 = 2 - \log 3 \approx 0.415$.
The sum of these two values is approximately $1.093$.

\item The highest probability outcome has probability $\frac38$, so the min-entropy is $-\log\frac38 = 3 - \log 3 = 1.415$.

\item The probability that $X =0$ is $\frac58$, so 
\[\Hmin(X) = -\log\frac58 = 3-\log 5 \approx 0.678\;.\]
Conditioned on $X = 0$, the most likely outcome for $Y$ is $Y = 0$ with probability $\frac35$. Conditioned on $X = 1$, the most likely outcome for $Y$ is $Y = 0$ with probability $\frac56$.
Then $P_\text{guess}(Y| X) = \frac58\cdot \frac35 + \frac38 \cdot \frac56 = \frac{11}{16}$. So $\Hmin(Y| X) = -\log \frac{11}{16} = 4 - \log 11 \approx 0.541$.
The sum of these two values is approximately $1.219$.
\end{enumerate}
\end{exercises}